from fastapi import FastAPI, UploadFile, File
from typing import List
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import torch, faiss, pandas as pd
from docx import Document
import io, pickle, os, datetime
import re

app = FastAPI(title="RAG Multi-file API", description="Upload nhi·ªÅu file v√† h·ªèi LLM")

# --------- Load local models ----------
llm_path = "./models/llm/llama2-7b-chat"
embed_path = "./models/embeddings/bge-m3"  # model embedding m·∫°nh ƒë√£ t·∫£i

tokenizer = AutoTokenizer.from_pretrained(llm_path)
model = AutoModelForCausalLM.from_pretrained(
    llm_path,
    device_map="auto",
    torch_dtype="auto"
)
embed_model = SentenceTransformer(embed_path)

# --------- File paths ----------
FAISS_PATH = "index.faiss"
DOCS_PATH = "documents.pkl"

# --------- Initialize or load existing index ----------
dimension = embed_model.get_sentence_embedding_dimension()
if os.path.exists(FAISS_PATH) and os.path.exists(DOCS_PATH):
    print("üîπ Loading existing FAISS index and documents...")
    index = faiss.read_index(FAISS_PATH)
    with open(DOCS_PATH, "rb") as f:
        documents = pickle.load(f)
else:
    print("üÜï Creating new FAISS index...")
    index = faiss.IndexFlatL2(dimension)
    documents = []

# --------- Helpers ----------
def read_txt(file_bytes: bytes):
    return file_bytes.decode("utf-8", errors="ignore")

def read_docx(file_bytes: bytes):
    doc = Document(io.BytesIO(file_bytes))
    return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])

def read_xlsx(file_bytes: bytes):
    excel = pd.read_excel(io.BytesIO(file_bytes), sheet_name=None)
    text_chunks = []
    for sheet_name, df in excel.items():
        text_chunks.append(f"Sheet: {sheet_name}")
        text_chunks.append(df.to_string(index=False))
    return "\n".join(text_chunks)

def chunk_text(text, chunk_size=300):
    words = text.split()
    return [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

def save_index_and_docs():
    faiss.write_index(index, FAISS_PATH)
    with open(DOCS_PATH, "wb") as f:
        pickle.dump(documents, f)
    print("üíæ ƒê√£ l∆∞u FAISS v√† documents th√†nh c√¥ng.")

# --------- API Models ----------
class Query(BaseModel):
    question: str
    max_new_tokens: int = 300

# --------- Upload nhi·ªÅu file ----------
@app.post("/upload_files")
async def upload_files(files: List[UploadFile] = File(...)):
    uploaded_info = []
    for file in files:
        ext = file.filename.split(".")[-1].lower()
        content = await file.read()

        if ext == "txt":
            text = read_txt(content)
        elif ext == "docx":
            text = read_docx(content)
        elif ext in ["xls", "xlsx"]:
            text = read_xlsx(content)
        else:
            uploaded_info.append({"file": file.filename, "status": "unsupported"})
            continue

        chunks = chunk_text(text)
        embeddings = embed_model.encode(chunks)
        index.add(embeddings)
        upload_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        for chunk in chunks:
            documents.append({
                "file": file.filename,
                "text": chunk,
                "upload_time": upload_time
            })

        uploaded_info.append({"file": file.filename, "chunks": len(chunks)})

    save_index_and_docs()
    return {
        "status": "uploaded",
        "total_files": len(uploaded_info),
        "files": uploaded_info,
        "total_documents": len(documents)
    }

# --------- Ask question ----------
# --------- Ask question ----------
@app.post("/ask")
def ask_question(query: Query):
    if len(documents) == 0:
        return {"error": "Ch∆∞a c√≥ t√†i li·ªáu n√†o, h√£y upload tr∆∞·ªõc."}

    # T·∫°o embedding cho c√¢u h·ªèi v√† t√¨m ƒëo·∫°n li√™n quan
    q_emb = embed_model.encode([query.question])
    D, I = index.search(q_emb, k=5)  # l·∫•y nhi·ªÅu ƒëo·∫°n h∆°n m·ªôt ch√∫t ƒë·ªÉ ƒë·ªß ng·ªØ c·∫£nh
    retrieved = "\n\n".join([documents[i]["text"] for i in I[0]])

    # Prompt t·ªëi ∆∞u
    prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch qu√¢n s·ª±, h√£y ƒë·ªçc k·ªπ NG·ªÆ C·∫¢NH sau ƒë√¢y v√† tr·∫£ l·ªùi C√ÇU H·ªéI.
- Tr√≠ch d·∫´n ƒë√∫ng th√¥ng tin trong t√†i li·ªáu, kh√¥ng suy di·ªÖn.
- N·∫øu t√†i li·ªáu c√≥ ƒëo·∫°n li√™n quan, h√£y t·ªïng h·ª£p l·∫°i th√†nh c√¢u vƒÉn r√µ r√†ng, s√∫c t√≠ch.
- N·∫øu kh√¥ng c√≥ th√¥ng tin, ch·ªâ c·∫ßn tr·∫£ l·ªùi: "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."

--- NG·ªÆ C·∫¢NH ---
{retrieved}

--- C√ÇU H·ªéI ---
{query.question}

--- TR·∫¢ L·ªúI (b·∫±ng ti·∫øng Vi·ªát) ---
"""

    # Sinh c√¢u tr·∫£ l·ªùi
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=min(query.max_new_tokens, 200),
        temperature=0.1,
        top_p=0.8
    )

    raw_answer = tokenizer.decode(
        outputs[0],
        skip_special_tokens=True,
        clean_up_tokenization_spaces=True
    )
    raw_answer = re.sub(r'([^\w\s])\1{5,}', '', raw_answer)
    raw_answer = re.sub(r'(\b\w+\b)(?:\s+\1){3,}', r'\1', raw_answer)
    # -------- L√†m s·∫°ch output, ch·ªâ gi·ªØ ph·∫ßn tr·∫£ l·ªùi th·∫≠t s·ª± --------
    # C·∫Øt ph·∫ßn sau nh√£n "--- TR·∫¢ L·ªúI"
    clean_answer = raw_answer.split("--- TR·∫¢ L·ªúI")[-1]
    clean_answer = clean_answer.replace("(b·∫±ng ti·∫øng Vi·ªát)", "")
    clean_answer = clean_answer.replace(":", "").strip()

    # N·∫øu m√¥ h√¨nh nh·∫Øc l·∫°i prompt, lo·∫°i b·ªè l·∫°i ph·∫ßn th·ª´a
    for tag in ["--- NG·ªÆ C·∫¢NH", "--- C√ÇU H·ªéI", "B·∫°n l√† m·ªôt chuy√™n gia"]:
        if tag in clean_answer:
            clean_answer = clean_answer.split(tag)[0].strip()

    # N·∫øu v·∫´n tr·ªëng, fallback b·∫±ng c√°ch l·∫•y 2 d√≤ng cu·ªëi c√πng
    if not clean_answer:
        lines = [l.strip() for l in raw_answer.splitlines() if l.strip()]
        clean_answer = " ".join(lines[-3:])

    # K·∫øt qu·∫£ g·ªçn g√†ng
    result = {"answer": clean_answer}

    # Th√™m th√¥ng tin debug n·∫øu c·∫ßn
    if hasattr(query, "debug") and query.debug:
        result["raw_answer"] = raw_answer
        result["context_used"] = retrieved[:600] + "..."

    return result


# --------- Reset to√†n b·ªô d·ªØ li·ªáu ----------
@app.post("/reset_index")
def reset_index():
    global index, documents
    index = faiss.IndexFlatL2(dimension)
    documents = []
    if os.path.exists(FAISS_PATH): os.remove(FAISS_PATH)
    if os.path.exists(DOCS_PATH): os.remove(DOCS_PATH)
    return {"status": "reset", "message": "ƒê√£ x√≥a to√†n b·ªô d·ªØ li·ªáu."}
