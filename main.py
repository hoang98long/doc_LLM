from fastapi import FastAPI, UploadFile, File
from typing import List, Optional
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import torch, faiss, pandas as pd
from docx import Document
import io, pickle, os, datetime, re

app = FastAPI(title="RAG Multi-file API", description="Upload nhi·ªÅu file v√† h·ªèi LLM v·ªõi prompt t·ªëi ∆∞u")

# --------- Load local models ----------
llm_path = "./models/llm/llama2-7b-chat"
embed_path = "./models/embeddings/bge-m3"  # ho·∫∑c intfloat/multilingual-e5-large

tokenizer = AutoTokenizer.from_pretrained(llm_path)
model = AutoModelForCausalLM.from_pretrained(
    llm_path,
    device_map="auto",
    torch_dtype="auto"
)
embed_model = SentenceTransformer(embed_path)

# --------- File paths ----------
FAISS_PATH = "index.faiss"
DOCS_PATH = "documents.pkl"

# --------- Initialize or load existing index ----------
dimension = embed_model.get_sentence_embedding_dimension()
if os.path.exists(FAISS_PATH) and os.path.exists(DOCS_PATH):
    print("üîπ Loading existing FAISS index and documents...")
    index = faiss.read_index(FAISS_PATH)
    with open(DOCS_PATH, "rb") as f:
        documents = pickle.load(f)
else:
    print("üÜï Creating new FAISS index...")
    index = faiss.IndexFlatL2(dimension)
    documents = []

# --------- Helpers ----------
def read_txt(file_bytes: bytes):
    return file_bytes.decode("utf-8", errors="ignore")

def read_docx(file_bytes: bytes):
    doc = Document(io.BytesIO(file_bytes))
    return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])

def read_xlsx(file_bytes: bytes):
    excel = pd.read_excel(io.BytesIO(file_bytes), sheet_name=None)
    text_chunks = []
    for sheet_name, df in excel.items():
        text_chunks.append(f"Sheet: {sheet_name}")
        text_chunks.append(df.to_string(index=False))
    return "\n".join(text_chunks)

def smart_chunk(text, chunk_size=350):
    """T√°ch vƒÉn b·∫£n th√†nh ƒëo·∫°n th√¥ng minh d·ª±a tr√™n ti√™u ƒë·ªÅ / ƒë·ªô d√†i."""
    # t√°ch theo ti√™u ƒë·ªÅ ho·∫∑c d√≤ng tr·ªëng
    parts = re.split(r'\n(?=\d+\.)|\n(?=\(\d+\))|(?=\n[A-Z])', text)
    chunks = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        words = p.split()
        if len(words) > chunk_size:
            for i in range(0, len(words), chunk_size):
                chunks.append(" ".join(words[i:i+chunk_size]))
        else:
            chunks.append(p)
    return chunks

def save_index_and_docs():
    faiss.write_index(index, FAISS_PATH)
    with open(DOCS_PATH, "wb") as f:
        pickle.dump(documents, f)
    print("üíæ ƒê√£ l∆∞u FAISS v√† documents th√†nh c√¥ng.")

# --------- API Models ----------
class Query(BaseModel):
    question: str
    max_new_tokens: int = 300
    debug: Optional[bool] = False  # n·∫øu True -> tr·∫£ th√™m raw_answer v√† context

# --------- Upload nhi·ªÅu file ----------
@app.post("/upload_files")
async def upload_files(files: List[UploadFile] = File(...)):
    uploaded_info = []
    for file in files:
        ext = file.filename.split(".")[-1].lower()
        content = await file.read()

        if ext == "txt":
            text = read_txt(content)
        elif ext == "docx":
            text = read_docx(content)
        elif ext in ["xls", "xlsx"]:
            text = read_xlsx(content)
        else:
            uploaded_info.append({"file": file.filename, "status": "unsupported"})
            continue

        chunks = smart_chunk(text)
        embeddings = embed_model.encode(chunks)
        index.add(embeddings)
        upload_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        for chunk in chunks:
            documents.append({
                "file": file.filename,
                "text": chunk,
                "upload_time": upload_time
            })

        uploaded_info.append({"file": file.filename, "chunks": len(chunks)})

    save_index_and_docs()
    return {
        "status": "uploaded",
        "total_files": len(uploaded_info),
        "files": uploaded_info,
        "total_documents": len(documents)
    }

# --------- Ask question ----------
@app.post("/ask")
def ask_question(query: Query):
    if len(documents) == 0:
        return {"error": "Ch∆∞a c√≥ t√†i li·ªáu n√†o, h√£y upload tr∆∞·ªõc."}

    q_emb = embed_model.encode([query.question])
    D, I = index.search(q_emb, k=8)  # l·∫•y nhi·ªÅu ƒëo·∫°n h∆°n
    retrieved = "\n\n".join([documents[i]["text"] for i in I[0]])

    # Prompt t·ªëi ∆∞u ti·∫øng Vi·ªát
    prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch qu√¢n s·ª±, h√£y ƒë·ªçc k·ªπ NG·ªÆ C·∫¢NH sau ƒë√¢y v√† tr·∫£ l·ªùi C√ÇU H·ªéI.
- Tr√≠ch d·∫´n ƒë√∫ng th√¥ng tin trong t√†i li·ªáu, kh√¥ng suy di·ªÖn.
- N·∫øu t√†i li·ªáu c√≥ ƒëo·∫°n li√™n quan, h√£y t·ªïng h·ª£p l·∫°i th√†nh c√¢u vƒÉn r√µ r√†ng, s√∫c t√≠ch.
- N·∫øu kh√¥ng c√≥ th√¥ng tin, ch·ªâ c·∫ßn tr·∫£ l·ªùi: "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."

--- NG·ªÆ C·∫¢NH ---
{retrieved}

--- C√ÇU H·ªéI ---
{query.question}

--- TR·∫¢ L·ªúI (b·∫±ng ti·∫øng Vi·ªát) ---
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=query.max_new_tokens,
        temperature=0.3,
        top_p=0.9
    )
    raw_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # -------- L√†m s·∫°ch output --------
    clean_answer = raw_text.split("--- TR·∫¢ L·ªúI")[-1]
    clean_answer = clean_answer.replace("(b·∫±ng ti·∫øng Vi·ªát)", "")
    clean_answer = clean_answer.replace(":", "").strip()
    if "--- NG·ªÆ C·∫¢NH" in clean_answer:
        clean_answer = clean_answer.split("--- NG·ªÆ C·∫¢NH")[0].strip()

    # K·∫øt qu·∫£ ƒë·∫ßu ra
    result = {"answer": clean_answer}

    if query.debug:
        result["context_used"] = retrieved[:600] + "..."
        result["raw_answer"] = raw_text

    return result

# --------- Reset to√†n b·ªô d·ªØ li·ªáu ----------
@app.post("/reset_index")
def reset_index():
    global index, documents
    index = faiss.IndexFlatL2(dimension)
    documents = []
    if os.path.exists(FAISS_PATH): os.remove(FAISS_PATH)
    if os.path.exists(DOCS_PATH): os.remove(DOCS_PATH)
    return {"status": "reset", "message": "ƒê√£ x√≥a to√†n b·ªô d·ªØ li·ªáu."}
